{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "k.A.I.speaker_v1.1.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyOanQ7cT5avkN7JPneRMYHB",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/kAIspeaker/k.A.I_speaker/blob/main/k_A_I_speaker_v1_1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TqWVs9ir0nUD",
        "outputId": "2af08566-1cf9-4bbb-a226-02c3876502ad"
      },
      "source": [
        "torch.cuda.is_available() "
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eAktGj29tILI"
      },
      "source": [
        "# !pip install sentencepiece\n",
        "\n",
        "!pip install 'git+https://github.com/SKTBrain/KoBERT.git#egg=kobert_tokenizer&subdirectory=kobert_hf'\n",
        "\n",
        "# !git clone https://github.com/SKTBrain/KoBERT.git\n",
        "# !cd KoBERT\n",
        "# !pip install -r requirements.txt\n",
        "\n",
        "!pip install mxnet\n",
        "!pip install gluonnlp pandas tqdm\n",
        "!pip install sentencepiece==0.1.91\n",
        "!pip install transformers==4.8.2\n",
        "# !pip install transformers==3# 버트 못가져옴\n",
        "!pip install pytorch==1.8.1\n",
        "\n",
        "from kobert_tokenizer import KoBERTTokenizer\n",
        "\n",
        "from transformers import BertModel\n",
        "\n",
        "import torch\n",
        "from torch import nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import gluonnlp as nlp\n",
        "import numpy as np\n",
        "from tqdm import tqdm, tqdm_notebook\n",
        "from transformers import AdamW\n",
        "from transformers.optimization import get_cosine_schedule_with_warmup\n",
        "\n",
        "import os\n",
        "os.environ['CUDA_LAUNCH_BLOCKING'] = \"1\"\n",
        "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\"\n",
        "\n",
        "# ##GPU 사용 시\n",
        "device = torch.device(\"cuda:0\")\n",
        "#구글드라이브 연동\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "import pandas as pd\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EGqB3HDasLiR"
      },
      "source": [
        "chatbot_data = pd.read_excel(\"/content/drive/MyDrive/food_labeling.xlsx\")\n",
        "len(chatbot_data)\n",
        "chatbot_data.sample(n=11)\n",
        "\n",
        "chatbot_data.loc[(chatbot_data['Status'] == \"결제\"), 'Status'] = 0  \n",
        "chatbot_data.loc[(chatbot_data['Status'] == \"메뉴판\"), 'Status'] = 1  \n",
        "chatbot_data.loc[(chatbot_data['Status'] == \"변경\"), 'Status'] = 2  \n",
        "chatbot_data.loc[(chatbot_data['Status'] == \"영수증\"), 'Status'] = 3  \n",
        "chatbot_data.loc[(chatbot_data['Status'] == \"재고_확인\"), 'Status'] = 4  \n",
        "chatbot_data.loc[(chatbot_data['Status'] == \"정보_가격\"), 'Status'] = 5  \n",
        "chatbot_data.loc[(chatbot_data['Status'] == \"정보_메뉴\"), 'Status'] = 6  \n",
        "chatbot_data.loc[(chatbot_data['Status'] == \"정보_제휴\"), 'Status'] = 7  \n",
        "chatbot_data.loc[(chatbot_data['Status'] == \"주문\"), 'Status'] = 8  \n",
        "chatbot_data.loc[(chatbot_data['Status'] == \"추천\"), 'Status'] = 9  \n",
        "chatbot_data.loc[(chatbot_data['Status'] == \"포인트_적립\"), 'Status'] = 10  \n",
        "\n",
        "data_list = []\n",
        "for q, label in zip(chatbot_data['Sentence'], chatbot_data['Status'])  :\n",
        "    data = []\n",
        "    data.append(q)\n",
        "    data.append(str(label))\n",
        "    data_list.append(data)\n",
        "\n",
        "print(len(data_list))\n",
        "print(data_list[1])\n",
        "print(data_list[-1])\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zhFQyPYEsLtm"
      },
      "source": [
        "#train & test 데이터로 나누기\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "dataset_train, dataset_test = train_test_split(data_list, test_size=0.25, random_state=0)\n",
        "\n",
        "print(len(dataset_train))\n",
        "print(len(dataset_test))\n",
        "print(dataset_train[0])\n",
        "print(dataset_test[0])\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GiWtaTF1sm34"
      },
      "source": [
        "class BERTDataset(Dataset):\n",
        "    def __init__(self, dataset, sent_idx, label_idx, bert_tokenizer,vocab, max_len,\n",
        "                 pad, pair):\n",
        "        transform = nlp.data.BERTSentenceTransform(bert_tokenizer,vocab=vocab,max_seq_length=max_len, pad=pad, pair=pair)\n",
        "        \n",
        "        self.sentences = [transform([i[sent_idx]]) for i in dataset]\n",
        "        self.labels = [np.int32(i[label_idx]) for i in dataset]\n",
        "\n",
        "    def __getitem__(self, i):\n",
        "        return (self.sentences[i] + (self.labels[i], ))\n",
        "\n",
        "    def __len__(self):\n",
        "        return (len(self.labels))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qyF88n6KsLvn"
      },
      "source": [
        "## Setting parameters\n",
        "max_len = 64\n",
        "batch_size = 64\n",
        "warmup_ratio = 0.1\n",
        "num_epochs = 10\n",
        "max_grad_norm = 1\n",
        "log_interval = 200\n",
        "learning_rate =  5e-5\n",
        "\n",
        "#토큰화\n",
        "tokenizer = KoBERTTokenizer.from_pretrained('skt/kobert-base-v1')\n",
        "vocab=tokenizer.vocab_file\n",
        "bertmodel = BertModel.from_pretrained('skt/kobert-base-v1')\n",
        "vocab = nlp.vocab.BERTVocab.from_sentencepiece(vocab, padding_token='[PAD]')\n",
        "\n",
        "# tok = nlp.data.BERTSPTokenizer(tokenizer, vocab, lower=False)#문제아\n",
        "tok = tokenizer.tokenize\n",
        "data_train = BERTDataset(dataset_train, 0, 1, tok, vocab, max_len, True, False)\n",
        "data_test = BERTDataset(dataset_test, 0, 1, tok, vocab, max_len, True, False)\n",
        "\n",
        "data_train[0]\n",
        "data_test[1]\n",
        "\n",
        "train_dataloader = torch.utils.data.DataLoader(data_train, batch_size=batch_size, num_workers=4)\n",
        "test_dataloader = torch.utils.data.DataLoader(data_test, batch_size=batch_size, num_workers=4)\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RbvKWHydsL0H"
      },
      "source": [
        "class BERTClassifier(nn.Module):\n",
        "    def __init__(self,\n",
        "                 bert,\n",
        "                 hidden_size = 768,\n",
        "                 num_classes=11,\n",
        "                 dr_rate=None,\n",
        "                 params=None):\n",
        "        super(BERTClassifier, self).__init__()\n",
        "        self.bert = bert\n",
        "        self.dr_rate = dr_rate\n",
        "                 \n",
        "        self.classifier = nn.Linear(hidden_size , num_classes)\n",
        "        if dr_rate:\n",
        "            self.dropout = nn.Dropout(p=dr_rate)\n",
        "    \n",
        "    def gen_attention_mask(self, token_ids, valid_length):\n",
        "        attention_mask = torch.zeros_like(token_ids)\n",
        "        for i, v in enumerate(valid_length):\n",
        "            attention_mask[i][:v] = 1\n",
        "        return attention_mask.float()\n",
        "\n",
        "    def forward(self, token_ids, valid_length, segment_ids):\n",
        "        attention_mask = self.gen_attention_mask(token_ids, valid_length)\n",
        "        \n",
        "        _, pooler = self.bert(input_ids = token_ids, token_type_ids = segment_ids.long(), attention_mask = attention_mask.float().to(token_ids.device), return_dict=False)\n",
        "        # _, pooler = self.bert(input_ids = token_ids, token_type_ids = segment_ids.long(), attention_mask = attention_mask.float().to(token_ids.device) )\n",
        "        if self.dr_rate:\n",
        "            out = self.dropout(pooler)\n",
        "        return self.classifier(out)\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xny7iqVwsL2O"
      },
      "source": [
        "model = BERTClassifier(bertmodel,  dr_rate=0.5).to(device)\n",
        "# Prepare optimizer and schedule (linear warmup and decay)\n",
        "no_decay = ['bias', 'LayerNorm.weight']\n",
        "optimizer_grouped_parameters = [\n",
        "    {'params': [p for n, p in model.named_parameters() if not any(nd in n for nd in no_decay)], 'weight_decay': 0.01},\n",
        "    {'params': [p for n, p in model.named_parameters() if any(nd in n for nd in no_decay)], 'weight_decay': 0.0}\n",
        "]\n",
        "\n",
        "optimizer = AdamW(optimizer_grouped_parameters, lr=learning_rate)\n",
        "loss_fn = nn.CrossEntropyLoss()\n",
        "\n",
        "t_total = len(train_dataloader) * num_epochs\n",
        "warmup_step = int(t_total * warmup_ratio)\n",
        "\n",
        "scheduler = get_cosine_schedule_with_warmup(optimizer, num_warmup_steps=warmup_step, num_training_steps=t_total)\n",
        "\n",
        "def calc_accuracy(X,Y):\n",
        "    max_vals, max_indices = torch.max(X, 1)\n",
        "    train_acc = (max_indices == Y).sum().data.cpu().numpy()/max_indices.size()[0]\n",
        "    return train_acc\n",
        "\n",
        "train_dataloader\n",
        "\n",
        "for e in range(num_epochs):\n",
        "    train_acc = 0.0\n",
        "    test_acc = 0.0\n",
        "    model.train()\n",
        "    for batch_id, (token_ids, valid_length, segment_ids, label) in enumerate(tqdm_notebook(train_dataloader)):\n",
        "        optimizer.zero_grad()\n",
        "        token_ids = token_ids.long().to(device)\n",
        "        segment_ids = segment_ids.long().to(device)\n",
        "        \n",
        "        valid_length= valid_length\n",
        "        label = label.long().to(device)\n",
        "        \n",
        "        out = model(token_ids, valid_length, segment_ids)\n",
        "        loss = loss_fn(out, label)\n",
        "        loss.backward()\n",
        "        torch.nn.utils.clip_grad_norm_(model.parameters(), max_grad_norm)\n",
        "        optimizer.step()\n",
        "        scheduler.step()  # Update learning rate schedule\n",
        "        train_acc += calc_accuracy(out, label)\n",
        "        if batch_id % log_interval == 0:\n",
        "            print(\"epoch {} batch id {} loss {} train acc {}\".format(e+1, batch_id+1, loss.data.cpu().numpy(), train_acc / (batch_id+1)))\n",
        "    print(\"epoch {} train acc {}\".format(e+1, train_acc / (batch_id+1)))\n",
        "    \n",
        "    model.eval()\n",
        "    for batch_id, (token_ids, valid_length, segment_ids, label) in enumerate(tqdm_notebook(test_dataloader)):\n",
        "        token_ids = token_ids.long().to(device)\n",
        "        segment_ids = segment_ids.long().to(device)\n",
        "        \n",
        "        valid_length= valid_length\n",
        "        label = label.long().to(device)\n",
        "        \n",
        "        out = model(token_ids, valid_length, segment_ids)\n",
        "        \n",
        "        test_acc += calc_accuracy(out, label)\n",
        "    print(\"epoch {} test acc {}\".format(e+1, test_acc / (batch_id+1)))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4KScwBDJ56PG"
      },
      "source": [
        "def predict(predict_sentence):\n",
        "\n",
        "    data = [predict_sentence, '0']\n",
        "    dataset_another = [data]\n",
        "\n",
        "    another_test = BERTDataset(dataset_another, 0, 1, tok, vocab, max_len, True, False)\n",
        "    test_dataloader = torch.utils.data.DataLoader(another_test, batch_size=batch_size, num_workers=5)\n",
        "    \n",
        "    model.eval()\n",
        "\n",
        "    for batch_id, (token_ids, valid_length, segment_ids, label) in enumerate(test_dataloader):\n",
        "        token_ids = token_ids.long().to(device)\n",
        "        segment_ids = segment_ids.long().to(device)\n",
        "\n",
        "        valid_length= valid_length\n",
        "        label = label.long().to(device)\n",
        "\n",
        "        out = model(token_ids, valid_length, segment_ids)\n",
        "\n",
        "\n",
        "        test_eval=[]\n",
        "        for i in out:\n",
        "            logits=i\n",
        "            logits = logits.detach().cpu().numpy()\n",
        "\n",
        "            if np.argmax(logits) == 0:\n",
        "                test_eval.append(\"결제\")\n",
        "            elif np.argmax(logits) == 1:\n",
        "                test_eval.append(\"메뉴판\")\n",
        "            elif np.argmax(logits) == 2:\n",
        "                test_eval.append(\"변경\")\n",
        "            elif np.argmax(logits) == 3:\n",
        "                test_eval.append(\"영수증\")\n",
        "            elif np.argmax(logits) == 4:\n",
        "                test_eval.append(\"재고_확인\")\n",
        "            elif np.argmax(logits) == 5:\n",
        "                test_eval.append(\"정보_가격\")\n",
        "            elif np.argmax(logits) == 6:\n",
        "                test_eval.append(\"정보_메뉴\")\n",
        "            elif np.argmax(logits) == 7:\n",
        "                test_eval.append(\"정보_제휴\")\n",
        "            elif np.argmax(logits) == 8:\n",
        "                test_eval.append(\"주문\")\n",
        "            elif np.argmax(logits) == 9:\n",
        "                test_eval.append(\"추천\")\n",
        "            elif np.argmax(logits) == 10:\n",
        "                test_eval.append(\"포인트_적립\")\n",
        "            \n",
        "        print(\">> 입력하신 내용은 \" + test_eval[0] + \" 에 대한 명령입니다.\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2BgqCq2P6gRy",
        "outputId": "ca636388-afaf-42cc-9913-0b8a8de760fb"
      },
      "source": [
        "#질문 무한반복하기! 0 입력시 종료\n",
        "end = 1\n",
        "while end == 1 :\n",
        "    sentence = input(\"하고싶은 말을 입력해주세요 : \")\n",
        "    if len(sentence) == 0 :\n",
        "        break\n",
        "    predict(sentence)\n",
        "    print(\"\\n\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "하고싶은 말을 입력해주세요 : 햄버거 하나 주세요\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:481: UserWarning: This DataLoader will create 5 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  cpuset_checked))\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            ">> 입력하신 내용은 주문 에 대한 명령입니다.\n",
            "\n",
            "\n",
            "하고싶은 말을 입력해주세요 : \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MJb27O7H9OkP"
      },
      "source": [
        "torch.save(model, 'k_A_I_speaker.pt')\n"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}